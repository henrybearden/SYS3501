{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrybearden/SYS3501/blob/main/hw4_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soSUFMuh6AK_"
      },
      "source": [
        "# Homework 4 - Understanding Machine Learning Pipeline\n",
        "\n",
        "- Read the code below and understand the whole process of building a machine learning pipeline\n",
        "- Answer the 5 questions in the markdown cells\n",
        "- Store your answers and submit your ipynb file via Canvas\n",
        "- You CAN use any resources including internet and GenAI tools (remember you can use ChatGPT to help you understand the code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAqYTeM6AK_"
      },
      "source": [
        "## An Machine Learning Pipeline for Titanic Dataset Survival Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK67DN_e6ALA",
        "outputId": "93ee42f6-cef7-473a-e9ff-f2a7e2b1f5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b0aa84c3b4e6>:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-1-b0aa84c3b4e6>:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
            "<ipython-input-1-b0aa84c3b4e6>:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['fare'].fillna(titanic['fare'].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fold Cross-Validation AccuracyScores: [0.77094972 0.8258427  0.87640449 0.80337079 0.83146067]\n",
            "Mean Accuracy Score: 0.8216056744711568\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load dataset\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Data cleaning\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "titanic['fare'].fillna(titanic['fare'].median(), inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "titanic['sex'] = LabelEncoder().fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = LabelEncoder().fit_transform(titanic['embarked'].astype(str))\n",
        "\n",
        "# Define raw features\n",
        "X_raw = titanic[['pclass', 'sex', 'age', 'fare', 'sibsp', 'parch', 'embarked']]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Feature engineering\n",
        "# Acutually some below features can be insightful, but some might be noise.\n",
        "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1 # family size = sibsp + parch + 1\n",
        "titanic['is_alone'] = (titanic['family_size'] == 1).astype(int) # is_alone = 1 if family size == 1, otherwise 0\n",
        "titanic['fare_bin'] = pd.qcut(titanic['fare'], 4, labels=[1, 2, 3, 4]).astype(int) # fare_bin = 1, 2, 3, 4; mapping fare into 4 bins\n",
        "titanic['age_fare_ratio'] = titanic['age'] / (titanic['fare'] + 1) # age_fare_ratio = age / (fare + 1)\n",
        "titanic['sibsp_parch_ratio'] = (titanic['sibsp'] + 1) / (titanic['parch'] + 1) # sibsp_parch_ratio = (sibsp + 1) / (parch + 1)\n",
        "titanic['age_class_interaction'] = titanic['age'] * titanic['pclass'] # age_class_interaction = age * pclass\n",
        "titanic['fare_per_family_member'] = titanic['fare'] / (titanic['family_size'] + 1) # fare_per_family_member = fare / (family_size + 1)\n",
        "\n",
        "# Combine features\n",
        "X_engineered = titanic[['pclass', 'sex', 'age', 'fare', 'embarked', 'family_size', 'is_alone',\n",
        "                       'fare_bin', 'age_fare_ratio', 'sibsp_parch_ratio', 'age_class_interaction',\n",
        "                       'fare_per_family_member']]\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define pipeline with feature selection and scaling with Random Forest model\n",
        "pipeline = Pipeline([\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=6)), # select 6 features with the highest F-values versus full features\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# Perform 5-fold cross-validation with the pipeline\n",
        "cv_scores = cross_val_score(pipeline, X_engineered, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Output cross-validation results\n",
        "print(\"5-Fold Cross-Validation AccuracyScores:\", cv_scores)\n",
        "print(\"Mean Accuracy Score:\", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zp6q_ZJ6ALA"
      },
      "source": [
        "#### Question 1: please describe the process of building this machine learning pipeline - step by step.\n",
        "\n",
        "Your answer here: First, you load and clean the dataset to handle missing values and other inconsistencies. Next, categorical variables are converted into numeric forms using LabelEncoder, making them suitable for machine learning algorithms. Next, you feature engineer new, potentially insightful features based on the raw data. You then define the machine learning pipeline with three main components: (1) feature selection, which retains only the top six features based on statistical analysis, (2) standard scaling with to normalize the data so that it is more suitable for the model, and (3) modeling with a RandomForestClassifier. Lastly, you use cross-validation with this pipeline to evaluate its performance, ensuring that each stage of the pipeline consistently applies to each fold of the data.\n",
        "\n",
        "#### Question 2: based on what you have learned in the lecture, please explain why we need to extract more features versus using raw features only.\n",
        "\n",
        "Your answer here: Extracting more features allows you to find more complex relationships in the data. For example, interactions between variables or variable-based-ratios can provide information that single features might not. Raw features often provide less detail, while engineered features can add detail by displaying more complicated relationships. Engineeering features like age_fare_ratio offers more nuanced interpretations of the data, potentially making patterns more visible and easier for the model to learn, meaning the model will also be more effective.\n",
        "\n",
        "#### Question 3: based on what you have learned in the lecture, please explain why we need feature selection.\n",
        "\n",
        "Your answer here: Feature selection helps you focus on the most useful features, reducing data \"noise.\" Complex data often contains extra, unhelpful information, which can negatively impact the performance of the model and increase computational load. By selecting only the top features, you simplify the model, reducing the risk of overfitting, and allowing the model to focus on the features that are most strongly related to the target variable. In this code, SelectKBest uses statistical tests to identify features that have the most predictive use, narrowing down the data to what is most relevant.\n",
        "\n",
        "#### Question 4: see printed results (accuracy scores across 5 folds versus averaged accuracy score), explain the benefits of using 5-fold CV versus one-time 80-20 split.\n",
        "\n",
        "Your answer here: The accuracy scores across the 5 folds (approximately 0.77, 0.83, 0.88, 0.80, and 0.83) show some variability in the model’s performance, but that the performance is relatively reliable depending on data split. When these scores are averaged, the cross-validation provides a more balanced estimate of model accuracy. The benefit of 5-fold cross-validation over a single 80-20 split lies in its depth and balance. Each data point is used in both training and testing, and the model is evaluated on many splits of data. This approach gives a more complete view of the model accuracy due to the broader collection and use of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if899RPR6ALA"
      },
      "source": [
        "## Compare the performance of different feature sets:\n",
        "- 1. raw features only;\n",
        "- 2. with engineered features;\n",
        "- 3. engineered features with feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKXkpPO66ALA",
        "outputId": "e9e00740-40a3-48ab-fc0d-690d1738c3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Experiment  Mean Score\n",
            "0                            Raw Features Only    0.803616\n",
            "1                      With Feature Extraction    0.811500\n",
            "2  With Feature Extraction & Feature Selection    0.821606\n"
          ]
        }
      ],
      "source": [
        "# Initialize a pipeline without feature selection\n",
        "pipeline_no_selection = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# 1. Raw features only\n",
        "cv_scores_raw = cross_val_score(pipeline_no_selection, X_raw, y, cv=5)\n",
        "\n",
        "# 2. Engineered features only (without feature selection)\n",
        "cv_scores_extraction = cross_val_score(pipeline_no_selection, X_engineered, y, cv=5)\n",
        "\n",
        "# 3. Engineered features with feature selection (global selection from previous block)\n",
        "cv_scores_full = cross_val_score(pipeline, X_engineered, y, cv=5)\n",
        "\n",
        "# Results summary\n",
        "results_df = pd.DataFrame({\n",
        "    'Experiment': [\n",
        "        'Raw Features Only',\n",
        "        'With Feature Extraction',\n",
        "        'With Feature Extraction & Feature Selection'\n",
        "    ],\n",
        "    'Mean Score': [\n",
        "        cv_scores_raw.mean(),\n",
        "        cv_scores_extraction.mean(),\n",
        "        cv_scores_full.mean()\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_AteML6ALA"
      },
      "source": [
        "#### Question 5: use formal language to explain printed results \"results_df\".\n",
        "\n",
        "Your answer here: The results show the impact of feature extraction and feature selection on model performance, measured by mean cross-validation score. Using only the raw features, the model had a mean score of 0.8036. With feature extraction, the model had an improved score of 0.8115. This tells us that engineered features offer useful information for the model. With both feature extraction and feature selection, the model had an even better score of 0.8216, suggesting that also focusing on the most informative features through feature selection leads to an even more effective model. These results highlight the importance of using feature extraction and selection to improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCafeBgz6ALA"
      },
      "source": [
        "## [Optional] Feature Importance Analysis\n",
        "- Note from lecturer:\n",
        "- Feature importance, in ML, measures how each feature contributes to the prediction accuracy of a machine learning model. It quantitatively reveals the influence of various factors on the model's predictive outcomes.\n",
        "- I found some final project groups proposed investigating the influence of certain factors on predictive outcomes but did not include a detailed methodological plan.\n",
        "- More specifically, descriptive analysis and visualizations are good for finding insights, but they are not enough to draw reliable quantitative conclusions.\n",
        "- For these groups, feature importance analysis might be a helpful approach to highlight the impact of different factors. I’ve attached the following code as a reference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "65n3dI0J6ALB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a54d35-b6c2-4b99-feab-ff81bb006a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Feature  Importance Original or Engineered\n",
            "4   age_class_interaction    0.280336             Engineered\n",
            "1                     sex    0.259849               Original\n",
            "5  fare_per_family_member    0.199735             Engineered\n",
            "2                    fare    0.176670               Original\n",
            "0                  pclass    0.054854               Original\n",
            "3                fare_bin    0.028557             Engineered\n"
          ]
        }
      ],
      "source": [
        "# Fit pipeline on the entire dataset to select features\n",
        "pipeline.fit(X_engineered, y)\n",
        "\n",
        "# Get selected features\n",
        "selected_features_indices = pipeline.named_steps['feature_selection'].get_support(indices=True)\n",
        "selected_features = X_engineered.columns[selected_features_indices]\n",
        "\n",
        "# Scale the data using the fitted scaler\n",
        "X_selected_scaled = pipeline.named_steps['scaling'].transform(X_engineered.iloc[:, selected_features_indices])\n",
        "\n",
        "# Train model on the entire dataset with selected features for global feature importance\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_selected_scaled, y)\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Display selected features and their importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Importance': feature_importances,\n",
        "    'Original or Engineered': ['Engineered' if f in ['family_size', 'is_alone', 'fare_bin',\n",
        "                                                     'age_fare_ratio', 'sibsp_parch_ratio',\n",
        "                                                     'age_class_interaction', 'fare_per_family_member']\n",
        "                               else 'Original' for f in selected_features]\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(feature_importance_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5VwCXKA6ALB"
      },
      "source": [
        "#### No question for feature importance :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIPR4hKE6ALB"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}